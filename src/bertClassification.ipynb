{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Literal, Union\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (['../data/train/H2.txt', '../data/train/C2.txt', '../data/train/F2.txt', '../data/train/C1.txt', '../data/train/G1.txt', '../data/train/A1.txt', '../data/train/A2.txt', '../data/train/H1.txt', '../data/train/F1.txt', '../data/train/B2.txt', '../data/train/D1.txt', '../data/train/E1.txt'], ['../data/train/G2.txt', '../data/train/E2.txt', '../data/train/D2.txt', '../data/train/B1.txt']), 2: (['../data/train/G2.txt', '../data/train/E2.txt', '../data/train/H2.txt', '../data/train/C2.txt', '../data/train/F2.txt', '../data/train/D2.txt', '../data/train/C1.txt', '../data/train/G1.txt', '../data/train/H1.txt', '../data/train/B2.txt', '../data/train/B1.txt', '../data/train/E1.txt'], ['../data/train/A1.txt', '../data/train/A2.txt', '../data/train/F1.txt', '../data/train/D1.txt']), 3: (['../data/train/G2.txt', '../data/train/E2.txt', '../data/train/C2.txt', '../data/train/D2.txt', '../data/train/C1.txt', '../data/train/A1.txt', '../data/train/A2.txt', '../data/train/H1.txt', '../data/train/F1.txt', '../data/train/B2.txt', '../data/train/D1.txt', '../data/train/B1.txt'], ['../data/train/H2.txt', '../data/train/F2.txt', '../data/train/G1.txt', '../data/train/E1.txt']), 4: (['../data/train/G2.txt', '../data/train/E2.txt', '../data/train/H2.txt', '../data/train/F2.txt', '../data/train/D2.txt', '../data/train/G1.txt', '../data/train/A1.txt', '../data/train/A2.txt', '../data/train/F1.txt', '../data/train/D1.txt', '../data/train/B1.txt', '../data/train/E1.txt'], ['../data/train/C2.txt', '../data/train/C1.txt', '../data/train/H1.txt', '../data/train/B2.txt'])}\n"
     ]
    }
   ],
   "source": [
    "from util import split_for_cross_validation\n",
    "\n",
    "Path_data = \"../data/train/*.txt\"\n",
    "folds = split_for_cross_validation(Path_data,4)\n",
    "print(folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G2': (284, 39.0, 121), 'G1': (224, 36.0, 172), 'F1': (109, 46.0, 133), 'F2': (245, 32.0, 94), 'A1': (393, 45.0, 385), 'A2': (228, 42.0, 221), 'C1': (349, 41.0, 242), 'C2': (681, 34.0, 299), 'B2': (101, 36.0, 138), 'B1': (217, 35.0, 154), 'E2': (102, 38.0, 85), 'E1': (364, 32.0, 121), 'D1': (307, 35.0, 138), 'H2': (221, 32.0, 89), 'D2': (106, 31.0, 100), 'H1': (193, 38.0, 99)}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "Path_data = \"../data/train/*.txt\"\n",
    "labels_dict = convert_to_labels_dict(Path_data)\n",
    "l2i, i2l = compute_l2i_and_i2l(flatten_labels_per_patent(labels_dict))\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "sentence_nb_length, long = check_sentence_length(Path_data,tokenizer,l2i)\n",
    "#sorted = {k: v for k, v in sorted(sentence_nb_length.items())}\n",
    "print(sentence_nb_length)\n",
    "print(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainerCallback, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "BASE_MODEL = \"bert-base-cased\"\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, id2label=i2l, label2id=l2i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
