{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Literal, Union\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_empty_label(labels_dict: Dict[str, List[str]]):\n",
    "    return '' not in labels_dict.keys() and '' not in labels_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_labels_dict(Path_data):\n",
    "    labels_dict = {}\n",
    "    for file_ in glob.glob(Path_data) :\n",
    "        labels = [sent.split(\"\\t\")[1].strip() for sent in open(file_).readlines()]\n",
    "        fileName = file_.split(\"/\")[3].split(\".\")[0]\n",
    "        labels_dict[fileName] = labels\n",
    "    if check_empty_label(labels_dict):\n",
    "        return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_labels_per_patent(labels_per_patent: Dict[str, List[str]]):\n",
    "    patents_names = list(labels_per_patent.keys())\n",
    "    patents_names.sort()\n",
    "    \n",
    "    labels_flattened = []\n",
    "    for patent_name in patents_names:\n",
    "        labels_flattened += labels_per_patent[patent_name]\n",
    "    return labels_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2i_and_i2l(labels):\n",
    "    l2i = {}\n",
    "    i2l = {}\n",
    "    for label in labels:\n",
    "        if label not in l2i:\n",
    "            idx = len(l2i.keys())\n",
    "            l2i[label] = idx\n",
    "            i2l[idx] = label\n",
    "    return l2i, i2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_data(path_document,encoded_label):\n",
    "    df = pd.DataFrame({'text':str(), 'label':int()}, index = [])\n",
    "    with open(path_document) as file_:\n",
    "        for line in file_.readlines():\n",
    "            text = line.split('\\t')[0]\n",
    "            label = line.split('\\t')[1].strip()\n",
    "            row_series = pd.Series((text,encoded_label[label]), index=[\"text\",\"label\"])\n",
    "            df = pd.concat([df,row_series.to_frame().T],\n",
    "                            ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence_length(Path_data,tokenizer,encoded_label):\n",
    "    import statistics\n",
    "    sentence_nb_length = {}\n",
    "    sentence_long = []\n",
    "    for path_document in glob.glob(Path_data):\n",
    "        df = load_input_data(path_document,encoded_label)\n",
    "        sentence_length = []\n",
    "        for idx, row in df.iterrows() : \n",
    "            tokenizedSentence = tokenizer.tokenize(row['text'])\n",
    "            if len(tokenizedSentence) > 512 :\n",
    "                sentence_long.append((path_document,idx))\n",
    "            sentence_length.append(len(tokenizedSentence))\n",
    "        sentence_nb_length[path_document.split(\"/\")[-1].split(\".\")[0]] = (len(df),\n",
    "                                                                          round(statistics.mean(sentence_length),0),\n",
    "                                                                          max(sentence_length))\n",
    "    return sentence_nb_length,sentence_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def split_for_cross_validation(Path_data,nb_fold):\n",
    "    \"\"\"\n",
    "    split data into train and validation for k fold validation\n",
    "    \"\"\"\n",
    "    folds = {}\n",
    "    files = glob.glob(Path_data)\n",
    "    kf = KFold(n_splits=nb_fold, shuffle=True, random_state=42)\n",
    "    fold_id = 1\n",
    "    for train_index, val_index in kf.split(files):\n",
    "        train_docs = [files[i] for i in train_index]\n",
    "        val_docs = [files[i] for i in val_index]\n",
    "        folds[fold_id] = (train_docs,val_docs)\n",
    "        fold_id += 1\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import split_for_cross_validation\n",
    "\n",
    "Path_data = \"../data/train/*.txt\"\n",
    "folds = split_for_cross_validation(Path_data,4)\n",
    "print(folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G2': (284, 39.0, 121), 'G1': (224, 36.0, 172), 'F1': (109, 46.0, 133), 'F2': (245, 32.0, 94), 'A1': (393, 45.0, 385), 'A2': (228, 42.0, 221), 'C1': (349, 41.0, 242), 'C2': (681, 34.0, 299), 'B2': (101, 36.0, 138), 'B1': (217, 35.0, 154), 'E2': (102, 38.0, 85), 'E1': (364, 32.0, 121), 'D1': (307, 35.0, 138), 'H2': (221, 32.0, 89), 'D2': (106, 31.0, 100), 'H1': (193, 38.0, 99)}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "Path_data = \"../data/train/*.txt\"\n",
    "labels_dict = convert_to_labels_dict(Path_data)\n",
    "l2i, i2l = compute_l2i_and_i2l(flatten_labels_per_patent(labels_dict))\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "sentence_nb_length, long = check_sentence_length(Path_data,tokenizer,l2i)\n",
    "#sorted = {k: v for k, v in sorted(sentence_nb_length.items())}\n",
    "print(sentence_nb_length)\n",
    "print(long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainerCallback, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "BASE_MODEL = \"bert-base-cased\"\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, id2label=i2l, label2id=l2i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
